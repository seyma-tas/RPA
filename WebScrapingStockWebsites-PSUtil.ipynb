{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A web scraper that seaches the news about the stock prices of a company and saves the results in a csv file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T16:51:14.817103Z",
     "start_time": "2021-02-11T16:51:14.781123Z"
    }
   },
   "outputs": [],
   "source": [
    "import psutil, os\n",
    "import pytest\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "#selenium\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T16:51:20.116844Z",
     "start_time": "2021-02-11T16:51:20.103171Z"
    },
    "code_folding": [
     0,
     7,
     12,
     19
    ]
   },
   "outputs": [],
   "source": [
    "def scrape():\n",
    "    \"\"\"\n",
    "    This function takes scrapes the web pages opened by Selenium, \n",
    "    returns headers, links, and texts of the webpages on a dictionary list.\n",
    "    \"\"\"\n",
    "    \n",
    "    pageInfo = []\n",
    "    try:\n",
    "      # wait for search results to be fetched\n",
    "       WebDriverWait(driver, 10).until(\n",
    "       EC.presence_of_element_located((By.CLASS_NAME, \"g\"))\n",
    "      )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        driver.quit()\n",
    "   # contains the search results\n",
    "    searchResults = driver.find_elements_by_class_name('g')\n",
    "\n",
    "   #aCOpRe\n",
    "    for result in searchResults:\n",
    "        element = result.find_element_by_css_selector('a') \n",
    "        link = element.get_attribute('href')\n",
    "        header = result.find_element_by_css_selector('h3').text\n",
    "        text = result.find_element_by_class_name('aCOpRe').text        \n",
    "    #append to the list    \n",
    "        pageInfo.append({\n",
    "          'header' : header, 'link' : link, 'text': text\n",
    "        })\n",
    "    return pageInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T16:51:26.184518Z",
     "start_time": "2021-02-11T16:51:21.525887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of the company you want to get news:  Apple\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/test/Desktop/RPA/RPA/selenium/chromedriver'\n",
    "website = \"https://www.google.com\"\n",
    "# set the keyword we want to search for\n",
    "company = input(\"Enter the name of the company you want to get news:  \")\n",
    "keyword = company +' stock news' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Stock News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T16:51:54.168870Z",
     "start_time": "2021-02-11T16:51:29.984211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU % used: 9.9\n",
      "memory % used: 71.1\n",
      "--------------------------------------------------------------------------------\n",
      "CPU % used: 20.1\n",
      "memory % used: 68.5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Measure the running time \n",
    "%%timeit\n",
    "\n",
    "#Measure the CPU and memory usage of the program\n",
    "#CPU Usage\n",
    "print('CPU % used:',psutil.cpu_percent())\n",
    "#physical memory usage\n",
    "print('memory % used:', psutil.virtual_memory()[2]) \n",
    "\n",
    "#Define the driver\n",
    "driver = Chrome(executable_path= path)\n",
    "driver.implicitly_wait(10)\n",
    "driver.get(website)\n",
    "\n",
    "\n",
    "# Find the search bar using it's name attribute value\n",
    "searchBar = driver.find_element_by_name('q')\n",
    "# Send keyword to the search bar followed by the enter \n",
    "searchBar.send_keys(keyword)\n",
    "searchBar.send_keys('\\n')\n",
    "#Wait\n",
    "time.sleep(10)\n",
    "\n",
    "# Number of pages to scrape\n",
    "numPages = 5\n",
    "# All the scraped data is stored in a dict list\n",
    "infoAll = []\n",
    "# Scraped data from page 1\n",
    "infoAll.extend(scrape())\n",
    "# Scraped data from 5 pages\n",
    "for i in range(0 , numPages - 1):\n",
    "    nextButton = driver.find_element_by_link_text('Next')\n",
    "    nextButton.click()\n",
    "    infoAll.extend(scrape())\n",
    "\n",
    "#Making a dataframe with the scraped information\n",
    "df = pd.DataFrame(infoAll)\n",
    "\n",
    "# Saving the data in a csv file\n",
    "fileName = keyword + '_' + str(numPages) + '.csv'\n",
    "df.to_csv(fileName)\n",
    "df = df.iloc[5:]\n",
    "\n",
    "#Measure the CPU and memory usage of the program\n",
    "#CPU Usage\n",
    "print('CPU % used:', psutil.cpu_percent())\n",
    "#physical memory usage\n",
    "print('memory % used:', psutil.virtual_memory()[2])\n",
    "print(40*'--')  \n",
    "# %timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the first 10 rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T16:54:00.321915Z",
     "start_time": "2021-02-11T16:53:54.835361Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.3 µs ± 1.91 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interview yarin oglen, Interview yapacagim yer bir finans sirketi, Pythonda RPA automation yapmak istiyorlar. Simdi Workfusion ile automation yapiyorlar. Beni de bot lari test etmek icin almak istiyorlar. Bir de biraz data analytics yaptiracaklar. Botlar soz konusu olunca isin icine AI girdigi icin data scientist hire ediyorlar. Benim oradaki connection im eger Python da bir bot yazip onu pytest ile test edebilirsen isi alirsin dedi. \n",
    "\n",
    "Boyle bir program yazdim. Sizin manuel olarak girdiginiz sirketin stock price larini bir csv file'a yaziyor. Biraz gelistirmek istiyorum. Sizin onerileriniz neler olabilir?\n",
    "1. Pytest ile bu programi test etsem nasil bir senaryo yazarim. Mesela kullanici sirket adi olarak \"££%$*\" yazsa program exception verse filan. try except ile bir hata mesaji koysam. \n",
    "2. Bu yaptigim CPU ve memory testleri dogru mu? psutil yerine guppy kullanmak daha mi iyi olur. Bu memory testlerinin logunu tutsam,memory_log.csv diye bir dosyada kaydetsem, soyle bir izlenim vermek istiyorum, yani ben cok basit bir program da yazsam onun testini yaparim loglarini tutarim vs. \n",
    "\n",
    "Software development tecrubem yok data science isi ariyorum normalde ama bu isi alirsam SDET te devam etmeyi dusunuyorum. Data science isi bulmak cok zor.\n",
    "\n",
    "Programi calistirirken executable_path='/Users/test/Desktop/RPA/RPA/selenium/chromedriver' kismini degistirmeniz lazim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measure the CPU and memory usage of the program\n",
    "#CPU Usage\n",
    "print('CPU % used:',psutil.cpu_percent())\n",
    "#physical memory usage\n",
    "print('memory % used:', psutil.virtual_memory()[2])\n",
    "print(40*'--')   \n",
    "    \n",
    "driver = Chrome(executable_path= path)\n",
    "driver.implicitly_wait(10)\n",
    "driver.get(website)\n",
    "\n",
    "# set the keyword you want to search for\n",
    "company = input(\"Enter the name of the company you want to get news:  \")\n",
    "# try exception throw\n",
    "print(\"You entered a wrong c.....\")\n",
    "\n",
    "keyword = company +' stock news' \n",
    "\n",
    "# we find the search bar using it's name attribute value\n",
    "searchBar = driver.find_element_by_name('q')\n",
    "# first we send our keyword to the search bar followed by the enter # key\n",
    "searchBar.send_keys(keyword)\n",
    "searchBar.send_keys('\\n')\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "# Number of pages to scrape\n",
    "numPages = 5\n",
    "# All the scraped data\n",
    "infoAll = []\n",
    "# Scraped data from page 1\n",
    "infoAll.extend(scrape())\n",
    "\n",
    "for i in range(0 , numPages - 1):\n",
    "    nextButton = driver.find_element_by_link_text('Next')\n",
    "    nextButton.click()\n",
    "    infoAll.extend(scrape())\n",
    "\n",
    "\n",
    "df = pd.DataFrame(infoAll)\n",
    "fileName = keyword + '_' + str(numPages) + '.csv'\n",
    "df.to_csv(fileName)\n",
    "log.info()\n",
    "df = df.iloc[5:]\n",
    "\n",
    "#Measure the CPU and memory usage of the program\n",
    "#CPU Usage\n",
    "print('CPU % used:', psutil.cpu_percent())\n",
    "#physical memory usage\n",
    "print('memory % used:', psutil.virtual_memory()[2])\n",
    "print(40*'--')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-11T02:25:17.485298Z",
     "start_time": "2021-02-11T02:25:13.928628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytest-html\n",
      "  Downloading https://files.pythonhosted.org/packages/75/36/6d3c3011751beefafcc7df6255c67b3739c0f8767691d21cbd7c9464aac8/pytest_html-3.1.1-py3-none-any.whl\n",
      "Collecting pytest-metadata (from pytest-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/12/bfb677aad996cc994efb9c61289a4994d60079587e85155738859fd3b68e/pytest_metadata-1.11.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pytest!=6.0.0,>=5.0 in /opt/anaconda3/lib/python3.7/site-packages (from pytest-html) (5.2.1)\n",
      "Requirement already satisfied: py>=1.5.0 in /opt/anaconda3/lib/python3.7/site-packages (from pytest!=6.0.0,>=5.0->pytest-html) (1.8.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.7/site-packages (from pytest!=6.0.0,>=5.0->pytest-html) (19.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/anaconda3/lib/python3.7/site-packages (from pytest!=6.0.0,>=5.0->pytest-html) (19.2.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /opt/anaconda3/lib/python3.7/site-packages (from pytest!=6.0.0,>=5.0->pytest-html) (7.2.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /opt/anaconda3/lib/python3.7/site-packages (from pytest!=6.0.0,>=5.0->pytest-html) (1.3.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /opt/anaconda3/lib/python3.7/site-packages (from pytest!=6.0.0,>=5.0->pytest-html) (0.13.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.7/site-packages (from pytest!=6.0.0,>=5.0->pytest-html) (0.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /opt/anaconda3/lib/python3.7/site-packages (from pytest!=6.0.0,>=5.0->pytest-html) (0.23)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from packaging->pytest!=6.0.0,>=5.0->pytest-html) (2.4.2)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from packaging->pytest!=6.0.0,>=5.0->pytest-html) (1.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest!=6.0.0,>=5.0->pytest-html) (0.6.0)\n",
      "Installing collected packages: pytest-metadata, pytest-html\n",
      "Successfully installed pytest-html-3.1.1 pytest-metadata-1.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytest-html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
